{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsenteeismAtWork = pd.read_csv('data/train_data.csv', index_col=0)\n",
    "AbsenteeismAtWork['Work load Average/day '] = [x.replace(',', '.') for x in AbsenteeismAtWork['Work load Average/day ']]\n",
    "AbsenteeismAtWork['Work load Average/day '] = AbsenteeismAtWork['Work load Average/day '].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = AbsenteeismAtWork[AbsenteeismAtWork.Absent==1]\n",
    "df_minority = AbsenteeismAtWork[AbsenteeismAtWork.Absent==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up-sample minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample with replacement\n",
    "\n",
    "Método mais simples que consiste em replicar aleatoriamente (com reposição) dados da classe minoritária até atingir ratio de 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    395\n",
      "0    395\n",
      "Name: Absent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=395,    \n",
    "                                 random_state=123) \n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(df_upsampled.Absent.value_counts())\n",
    "\n",
    "df_upsampled_data = df_upsampled.drop('Absent', 1)\n",
    "df_upsampled_target = df_upsampled['Absent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE - Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    395\n",
      "0    395\n",
      "Name: Absent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_sample(data, target)\n",
    "\n",
    "print(y_sm.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down-sample majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    105\n",
      "0    105\n",
      "Name: Absent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=105,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "print(df_downsampled.Absent.value_counts())\n",
    "\n",
    "df_downsampled_data = df_downsampled.drop('Absent', 1)\n",
    "df_downsampled_target = df_downsampled['Absent']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    360\n",
       "0    105\n",
       "Name: Absent, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl= tl.fit_sample(data, target)\n",
    "\n",
    "y_tl.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    105\n",
       "0    105\n",
       "Name: Absent, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids(sampling_strategy='majority')\n",
    "X_cc, y_cc = cc.fit_sample(data, target)\n",
    "y_cc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize(data):\n",
    "    featuresToDiscretize = ['Transportation expense', 'Distance from Residence to Work', 'Service time', 'Age', 'Work load Average/day ', 'Hit target', 'Weight', 'Height', 'Body mass index']\n",
    "    discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "    data[featuresToDiscretize] = discretizer.fit_transform(data[featuresToDiscretize])\n",
    "    return data;\n",
    "\n",
    "\n",
    "df_upsampled = discretize(df_upsampled)\n",
    "X_sm = discretize(X_sm)\n",
    "df_downsampled = discretize(df_downsampled)\n",
    "X_tl = discretize(X_tl)\n",
    "X_cc = discretize(X_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest (Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### SelectKBest for randomly upsampled data #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(2, 'Day of the week')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(11, 'Education')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### SelectKBest for SMOTE #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(3, 'Seasons')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(11, 'Education')\n",
      "\t(15, 'Pet')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### SelectKBest for randomly downsampled data #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(16, 'Weight')\n",
      "\t(17, 'Height')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\n",
      "##### SelectKBest for Tomek Links #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(6, 'Service time')\n",
      "\t(7, 'Age')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(12, 'Son')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(11, 'Education')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### SelectKBest for Cluster Centroids #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(12, 'Son')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(11, 'Education')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(17, 'Height')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "def createKBestSelector(data, target, numFeatures):\n",
    "    kbest_selector = SelectKBest(f_classif, k=numFeatures)\n",
    "    kbest_selector = kbest_selector.fit(data, target)\n",
    "    return kbest_selector;\n",
    "\n",
    "\n",
    "def getPickedFeatures(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    selected_features_names = list(data.columns[selected_features_index])\n",
    "    return selected_features_names;\n",
    "\n",
    "\n",
    "def getDroppedFeatures(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    dropped_features_names = list(data.columns[dropped_features_index])\n",
    "    return dropped_features_names;\n",
    "\n",
    "\n",
    "def printFeatureSelection(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    selected_features_names = zip(selected_features_index,  list(data.columns[selected_features_index]))\n",
    "    dropped_features_names = zip(dropped_features_index, list(data.columns[dropped_features_index]))\n",
    "\n",
    "    print(\"Features mantidas:\")\n",
    "    for cn in selected_features_names:\n",
    "        print(\"\\t\" + str(cn))\n",
    "\n",
    "    print(\"Features eliminadas:\")\n",
    "    for cn in dropped_features_names:\n",
    "        print(\"\\t\" + str(cn))\n",
    "    return;\n",
    "\n",
    "numFeaturesToSelect = 12\n",
    "\n",
    "print(\"##### SelectKBest for randomly upsampled data #####\")\n",
    "df_upsampled_kbest_selector =  createKBestSelector(df_upsampled_data, df_upsampled_target, numFeaturesToSelect)\n",
    "printFeatureSelection(df_upsampled_kbest_selector, df_upsampled_data)\n",
    "\n",
    "print(\"\\n##### SelectKBest for SMOTE #####\")\n",
    "smote_kbest_selector =  createKBestSelector(X_sm, y_sm, numFeaturesToSelect)\n",
    "printFeatureSelection(smote_kbest_selector, X_sm)\n",
    "\n",
    "\n",
    "print(\"\\n##### SelectKBest for randomly downsampled data #####\")\n",
    "df_downsampled_kbest_selector =  createKBestSelector(df_downsampled_data, df_downsampled_target, numFeaturesToSelect)\n",
    "printFeatureSelection(df_downsampled_kbest_selector, df_downsampled_data)\n",
    "\n",
    "print(\"\\n##### SelectKBest for Tomek Links #####\")\n",
    "tomek_kbest_selector =  createKBestSelector(X_tl, y_tl, numFeaturesToSelect)\n",
    "printFeatureSelection(tomek_kbest_selector, X_tl)\n",
    "\n",
    "\n",
    "print(\"\\n##### SelectKBest for Cluster Centroids #####\")\n",
    "cluster_kbest_selector =  createKBestSelector(X_cc, y_cc, numFeaturesToSelect)\n",
    "printFeatureSelection(cluster_kbest_selector, X_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (Wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### RFE with Logistic Regression for randomly upsampled data #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(6, 'Service time')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(17, 'Height')\n",
      "##### RFE with Linear SVC for randomly upsampled data #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(6, 'Service time')\n",
      "\t(7, 'Age')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(9, 'Hit target')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### RFE with Logistic Regression for SMOTE #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mantidas:\n",
      "\t(3, 'Seasons')\n",
      "\t(6, 'Service time')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### RFE with Linear SVC for SMOTE #####\n",
      "Features mantidas:\n",
      "\t(2, 'Day of the week')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(16, 'Weight')\n",
      "\t(17, 'Height')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(15, 'Pet')\n",
      "\n",
      "##### RFE with Logistic Regression for randomly downsampled data #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(16, 'Weight')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### RFE with Linear SVC for randomly downsampled data #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(7, 'Age')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(17, 'Height')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(9, 'Hit target')\n",
      "\n",
      "##### RFE with Logistic Regression for Tomek Links #####\n",
      "Features mantidas:\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(13, 'Social drinker')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### RFE with Linear SVC for Tomek Links #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(6, 'Service time')\n",
      "\t(7, 'Age')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(3, 'Seasons')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(9, 'Hit target')\n",
      "\t(12, 'Son')\n",
      "\t(17, 'Height')\n",
      "\n",
      "##### RFE with Logistic Regression for Cluster Centroids #####\n",
      "Features mantidas:\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(17, 'Height')\n",
      "Features eliminadas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(18, 'Body mass index')\n",
      "\n",
      "##### RFE with Linear SVC for Cluster Centroids #####\n",
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(11, 'Education')\n",
      "\t(12, 'Son')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(16, 'Weight')\n",
      "\t(17, 'Height')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(9, 'Hit target')\n",
      "\t(13, 'Social drinker')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "print(\"##### RFE with Logistic Regression for randomly upsampled data #####\")\n",
    "df_upsampled_rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "df_upsampled_rfe_log_selector = df_upsampled_rfe_log_selector.fit(df_upsampled_data, df_upsampled_target)\n",
    "printFeatureSelection(df_upsampled_rfe_log_selector, df_upsampled_data)\n",
    "\n",
    "print(\"##### RFE with Linear SVC for randomly upsampled data #####\")\n",
    "df_upsampled_rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "df_upsampled_rfe_svc_selector = df_upsampled_rfe_svc_selector.fit(df_upsampled_data, df_upsampled_target)\n",
    "printFeatureSelection(df_upsampled_rfe_svc_selector, df_upsampled_data)\n",
    "\n",
    "print(\"\\n##### RFE with Logistic Regression for SMOTE #####\")\n",
    "smote_rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "smote_rfe_log_selector = smote_rfe_log_selector.fit(data, target)\n",
    "printFeatureSelection(smote_rfe_log_selector, X_sm)\n",
    "\n",
    "print(\"\\n##### RFE with Linear SVC for SMOTE #####\")\n",
    "smote_rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "smote_rfe_svc_selector = smote_rfe_svc_selector.fit(X_sm, y_sm)\n",
    "printFeatureSelection(smote_rfe_svc_selector, X_sm)\n",
    "\n",
    "\n",
    "print(\"\\n##### RFE with Logistic Regression for randomly downsampled data #####\")\n",
    "df_downsampled_rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "df_downsampled_rfe_log_selector = df_downsampled_rfe_log_selector.fit(df_downsampled_data, df_downsampled_target)\n",
    "printFeatureSelection(df_downsampled_rfe_log_selector, df_downsampled_data)\n",
    "\n",
    "\n",
    "print(\"\\n##### RFE with Linear SVC for randomly downsampled data #####\")\n",
    "df_downsampled_rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "df_downsampled_rfe_svc_selector = df_downsampled_rfe_svc_selector.fit(df_downsampled_data, df_downsampled_target)\n",
    "printFeatureSelection(df_downsampled_rfe_svc_selector, df_downsampled_data)\n",
    "\n",
    "\n",
    "print(\"\\n##### RFE with Logistic Regression for Tomek Links #####\")\n",
    "tomek_rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "tomek_rfe_log_selector = tomek_rfe_log_selector.fit(X_tl, y_tl)\n",
    "printFeatureSelection(tomek_rfe_log_selector, X_tl)\n",
    "\n",
    "\n",
    "print(\"\\n##### RFE with Linear SVC for Tomek Links #####\")\n",
    "tomek_rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "tomek_rfe_svc_selector = tomek_rfe_svc_selector.fit(X_tl, y_tl)\n",
    "printFeatureSelection(tomek_rfe_svc_selector, X_tl)\n",
    "\n",
    "\n",
    "print(\"\\n##### RFE with Logistic Regression for Cluster Centroids #####\")\n",
    "cluster_rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "cluster_rfe_log_selector = cluster_rfe_log_selector.fit(X_cc, y_cc)\n",
    "printFeatureSelection(cluster_rfe_log_selector, X_cc)\n",
    "\n",
    "\n",
    "print(\"\\n##### RFE with Linear SVC for Cluster Centroids #####\")\n",
    "cluster_rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "cluster_rfe_svc_selector = cluster_rfe_svc_selector.fit(X_cc, y_cc)\n",
    "printFeatureSelection(cluster_rfe_svc_selector, X_cc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model types that will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Target var is 'Absent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = AbsenteeismAtWork['Absent']\n",
    "\n",
    "df_upsampled_kbest_selected = df_upsampled_kbest_selector.transform(df_upsampled_data)\n",
    "smote_kbest_selected = smote_kbest_selector.transform(X_sm)\n",
    "df_downsampled_kbest_selected = df_downsampled_kbest_selector.transform(df_downsampled_data)\n",
    "tomek_kbest_selected = tomek_kbest_selector.transform(X_tl)\n",
    "cluster_kbest_selected = cluster_kbest_selector.transform(X_cc)\n",
    "\n",
    "df_upsampled_rfe_log_selected = df_upsampled_rfe_log_selector.transform(df_upsampled_data)\n",
    "df_upsampled_rfe_svc_selected = df_upsampled_rfe_svc_selector.transform(df_upsampled_data)\n",
    "\n",
    "smote_rfe_log_selected = smote_rfe_log_selector.transform(X_sm) \n",
    "smote_rfe_svc_selected = smote_rfe_svc_selector.transform(X_sm)\n",
    "\n",
    "df_downsampled_rfe_log_selected = df_downsampled_rfe_log_selector.transform(df_downsampled_data)\n",
    "df_downsampled_rfe_svc_selected = df_downsampled_rfe_svc_selector.transform(df_downsampled_data)\n",
    "\n",
    "tomek_rfe_log_selected = tomek_rfe_log_selector.transform(X_tl)\n",
    "tomek_rfe_svc_selected = tomek_rfe_svc_selector.transform(X_tl)\n",
    "\n",
    "cluster_rfe_log_selected = cluster_rfe_log_selector.transform(X_cc)\n",
    "cluster_rfe_svc_selected = cluster_rfe_svc_selector.transform(X_cc)\n",
    "\n",
    "\n",
    "\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "# muita atencao a estes passos, podem surgir incoerencias entre teste/treino devido aos pre processamentos aplicados\n",
    "def preprocessTestSet(X_test, selector, data):\n",
    "    X_test_processed = discretize(X_test)\n",
    "    X_test_processed = X_test_processed.drop(getDroppedFeatures(selector, data), axis=1)\n",
    "    return X_test_processed;\n",
    "\n",
    "X_test1 = preprocessTestSet(X_test, df_upsampled_kbest_selector, df_upsampled_data)\n",
    "X_test2 = preprocessTestSet(X_test, smote_kbest_selector, X_sm)\n",
    "X_test3 = preprocessTestSet(X_test, df_downsampled_kbest_selector, df_downsampled_data)\n",
    "X_test4 = preprocessTestSet(X_test, tomek_kbest_selector, X_tl)\n",
    "X_test5 = preprocessTestSet(X_test, cluster_kbest_selector, X_cc)\n",
    "X_test6 = preprocessTestSet(X_test, df_upsampled_rfe_log_selector, df_upsampled_data)\n",
    "X_test7 = preprocessTestSet(X_test, df_upsampled_rfe_svc_selector, df_upsampled_data)\n",
    "X_test8 = preprocessTestSet(X_test, smote_rfe_log_selector, X_sm)\n",
    "X_test9 = preprocessTestSet(X_test, smote_rfe_svc_selector, X_sm)\n",
    "X_test10 = preprocessTestSet(X_test, df_downsampled_rfe_log_selector, df_downsampled_data)\n",
    "X_test11 = preprocessTestSet(X_test, df_downsampled_rfe_svc_selector, df_downsampled_data)\n",
    "X_test12 = preprocessTestSet(X_test, tomek_rfe_log_selector, X_tl)\n",
    "X_test13 = preprocessTestSet(X_test, tomek_rfe_svc_selector, X_tl)\n",
    "X_test14 = preprocessTestSet(X_test, cluster_rfe_log_selector, X_cc)\n",
    "X_test15 = preprocessTestSet(X_test, cluster_rfe_svc_selector, X_cc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn3 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn4 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "rbfSVC1 = SVC()\n",
    "rbfSVC2 = SVC()\n",
    "rbfSVC3 = SVC()\n",
    "rbfSVC4 = SVC()\n",
    "rbfSVC5 = SVC()\n",
    "\n",
    "log1 = LogisticRegression()\n",
    "log2 = LogisticRegression()\n",
    "log3 = LogisticRegression()\n",
    "log4 = LogisticRegression()\n",
    "log5 = LogisticRegression()\n",
    "\n",
    "linearSVC1 = SVC(kernel='linear')\n",
    "linearSVC2 = SVC(kernel='linear')\n",
    "linearSVC3 = SVC(kernel='linear')\n",
    "linearSVC4 = SVC(kernel='linear')\n",
    "linearSVC5 = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1.fit(df_upsampled_kbest_selected, df_upsampled_target)\n",
    "knn2.fit(smote_kbest_selected, y_sm)\n",
    "knn3.fit(df_downsampled_kbest_selected, df_downsampled_target)\n",
    "knn4.fit(tomek_kbest_selected, y_tl)\n",
    "knn5.fit(cluster_kbest_selected, y_cc)\n",
    "\n",
    "rbfSVC1.fit(df_upsampled_kbest_selected, df_upsampled_target)\n",
    "rbfSVC2.fit(smote_kbest_selected, y_sm)\n",
    "rbfSVC3.fit(df_downsampled_kbest_selected, df_downsampled_target)\n",
    "rbfSVC4.fit(tomek_kbest_selected, y_tl)\n",
    "rbfSVC5.fit(cluster_kbest_selected, y_cc)\n",
    "\n",
    "log1.fit(df_upsampled_rfe_log_selected, df_upsampled_target)\n",
    "log2.fit(smote_rfe_log_selected, y_sm)\n",
    "log3.fit(df_downsampled_rfe_log_selected, df_downsampled_target)\n",
    "log4.fit(tomek_rfe_log_selected, y_tl)\n",
    "log5.fit(cluster_rfe_log_selected, y_cc)\n",
    "\n",
    "linearSVC1.fit(df_upsampled_rfe_svc_selected, df_upsampled_target)\n",
    "linearSVC2.fit(smote_rfe_svc_selected, y_sm)\n",
    "linearSVC3.fit(df_downsampled_rfe_svc_selected, df_downsampled_target)\n",
    "linearSVC4.fit(tomek_rfe_svc_selected, y_tl)\n",
    "linearSVC5.fit(cluster_rfe_svc_selected, y_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Prediction and Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy knn_kbest_up_pred: 0.8166666666666667\n",
      "Accuracy knn_kbest_sm_pred: 0.6291666666666667\n",
      "Accuracy knn_kbest_down_pred: 0.7\n",
      "Accuracy knn_kbest_tl_pred: 0.7541666666666667\n",
      "Accuracy knn_kbest_cc_pred: 0.5291666666666667\n",
      "Accuracy rbf_kbest_up_pred: 0.8166666666666667\n",
      "Accuracy rbf_kbest_sm_pred: 0.7083333333333334\n",
      "Accuracy rbf_kbest_down_pred: 0.18333333333333332\n",
      "Accuracy rbf_kbest_tl_pred: 0.7916666666666666\n",
      "Accuracy rbf_kbest_cc_pred: 0.5666666666666667\n",
      "Accuracy log_rfe_up_pred: 0.20833333333333334\n",
      "Accuracy log_rfe_sm_pred: 0.5666666666666667\n",
      "Accuracy log_rfe_down_pred: 0.18333333333333332\n",
      "Accuracy log_rfe_tl_pred: 0.7875\n",
      "Accuracy log_rfe_cc_pred: 0.4666666666666667\n",
      "Accuracy linear_rfe_up_pred: 0.8166666666666667\n",
      "Accuracy linear_rfe_sm_pred: 0.6041666666666666\n",
      "Accuracy linear_rfe_down_pred: 0.8166666666666667\n",
      "Accuracy linear_rfe_tl_pred: 0.8083333333333333\n",
      "Accuracy linear_rfe_cc_pred: 0.5083333333333333\n"
     ]
    }
   ],
   "source": [
    "knn_kbest_up_pred = knn1.predict(X_test1)\n",
    "knn_kbest_sm_pred = knn2.predict(X_test2)\n",
    "knn_kbest_down_pred = knn3.predict(X_test3)\n",
    "knn_kbest_tl_pred = knn4.predict(X_test4)\n",
    "knn_kbest_cc_pred = knn5.predict(X_test5)\n",
    "\n",
    "rbf_kbest_up_pred = rbfSVC1.predict(X_test1)\n",
    "rbf_kbest_sm_pred = rbfSVC2.predict(X_test2)\n",
    "rbf_kbest_down_pred = rbfSVC3.predict(X_test3)\n",
    "rbf_kbest_tl_pred = rbfSVC4.predict(X_test4)\n",
    "rbf_kbest_cc_pred = rbfSVC5.predict(X_test5)\n",
    "\n",
    "log_rfe_up_pred = log1.predict(X_test6)\n",
    "log_rfe_sm_pred = log2.predict(X_test8)\n",
    "log_rfe_down_pred = log3.predict(X_test10)\n",
    "log_rfe_tl_pred = log4.predict(X_test12)\n",
    "log_rfe_cc_pred = log5.predict(X_test14)\n",
    "\n",
    "linear_rfe_up_pred = linearSVC1.predict(X_test7)\n",
    "linear_rfe_sm_pred = linearSVC2.predict(X_test9)\n",
    "linear_rfe_down_pred = linearSVC3.predict(X_test11)\n",
    "linear_rfe_tl_pred = linearSVC4.predict(X_test13)\n",
    "linear_rfe_cc_pred = linearSVC5.predict(X_test15)\n",
    "\n",
    "print(\"Accuracy knn_kbest_up_pred: \" + str(accuracy_score(y_test, knn_kbest_up_pred)))\n",
    "print(\"Accuracy knn_kbest_sm_pred: \" + str(accuracy_score(y_test, knn_kbest_sm_pred)))\n",
    "print(\"Accuracy knn_kbest_down_pred: \" + str(accuracy_score(y_test, knn_kbest_down_pred)))\n",
    "print(\"Accuracy knn_kbest_tl_pred: \" + str(accuracy_score(y_test, knn_kbest_tl_pred)))\n",
    "print(\"Accuracy knn_kbest_cc_pred: \" + str(accuracy_score(y_test, knn_kbest_cc_pred)))\n",
    "\n",
    "print(\"Accuracy rbf_kbest_up_pred: \" + str(accuracy_score(y_test, rbf_kbest_up_pred)))\n",
    "print(\"Accuracy rbf_kbest_sm_pred: \" + str(accuracy_score(y_test, rbf_kbest_sm_pred)))\n",
    "print(\"Accuracy rbf_kbest_down_pred: \" + str(accuracy_score(y_test, rbf_kbest_down_pred)))\n",
    "print(\"Accuracy rbf_kbest_tl_pred: \" + str(accuracy_score(y_test, rbf_kbest_tl_pred)))\n",
    "print(\"Accuracy rbf_kbest_cc_pred: \" + str(accuracy_score(y_test, rbf_kbest_cc_pred)))\n",
    "\n",
    "print(\"Accuracy log_rfe_up_pred: \" + str(accuracy_score(y_test, log_rfe_up_pred)))\n",
    "print(\"Accuracy log_rfe_sm_pred: \" + str(accuracy_score(y_test, log_rfe_sm_pred)))\n",
    "print(\"Accuracy log_rfe_down_pred: \" + str(accuracy_score(y_test, log_rfe_down_pred)))\n",
    "print(\"Accuracy log_rfe_tl_pred: \" + str(accuracy_score(y_test, log_rfe_tl_pred)))\n",
    "print(\"Accuracy log_rfe_cc_pred: \" + str(accuracy_score(y_test, log_rfe_cc_pred)))\n",
    "\n",
    "print(\"Accuracy linear_rfe_up_pred: \" + str(accuracy_score(y_test, linear_rfe_up_pred)))\n",
    "print(\"Accuracy linear_rfe_sm_pred: \" + str(accuracy_score(y_test, linear_rfe_sm_pred)))\n",
    "print(\"Accuracy linear_rfe_down_pred: \" + str(accuracy_score(y_test, linear_rfe_down_pred)))\n",
    "print(\"Accuracy linear_rfe_tl_pred: \" + str(accuracy_score(y_test, linear_rfe_tl_pred)))\n",
    "print(\"Accuracy linear_rfe_cc_pred: \" + str(accuracy_score(y_test, linear_rfe_cc_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "##### knn_kbest_up_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        44\n",
      "           1       0.82      1.00      0.90       196\n",
      "\n",
      "    accuracy                           0.82       240\n",
      "   macro avg       0.41      0.50      0.45       240\n",
      "weighted avg       0.67      0.82      0.73       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### knn_kbest_sm_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.48      0.32        44\n",
      "           1       0.85      0.66      0.74       196\n",
      "\n",
      "    accuracy                           0.63       240\n",
      "   macro avg       0.55      0.57      0.53       240\n",
      "weighted avg       0.74      0.63      0.67       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### knn_kbest_down_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.32      0.28        44\n",
      "           1       0.84      0.79      0.81       196\n",
      "\n",
      "    accuracy                           0.70       240\n",
      "   macro avg       0.54      0.55      0.55       240\n",
      "weighted avg       0.73      0.70      0.71       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### knn_kbest_tl_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.07      0.09        44\n",
      "           1       0.81      0.91      0.86       196\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.48      0.49      0.48       240\n",
      "weighted avg       0.69      0.75      0.72       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### knn_kbest_cc_pred  #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.70      0.35        44\n",
      "           1       0.88      0.49      0.63       196\n",
      "\n",
      "    accuracy                           0.53       240\n",
      "   macro avg       0.56      0.60      0.49       240\n",
      "weighted avg       0.76      0.53      0.58       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### rbf_kbest_up_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        44\n",
      "           1       0.82      1.00      0.90       196\n",
      "\n",
      "    accuracy                           0.82       240\n",
      "   macro avg       0.41      0.50      0.45       240\n",
      "weighted avg       0.67      0.82      0.73       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### rbf_kbest_sm_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.16      0.17        44\n",
      "           1       0.81      0.83      0.82       196\n",
      "\n",
      "    accuracy                           0.71       240\n",
      "   macro avg       0.49      0.50      0.49       240\n",
      "weighted avg       0.70      0.71      0.70       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  rbf_kbest_down_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      1.00      0.31        44\n",
      "           1       0.00      0.00      0.00       196\n",
      "\n",
      "    accuracy                           0.18       240\n",
      "   macro avg       0.09      0.50      0.15       240\n",
      "weighted avg       0.03      0.18      0.06       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  rbf_kbest_tl_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.11      0.17        44\n",
      "           1       0.83      0.94      0.88       196\n",
      "\n",
      "    accuracy                           0.79       240\n",
      "   macro avg       0.57      0.53      0.52       240\n",
      "weighted avg       0.73      0.79      0.75       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  rbf_kbest_cc_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.43      0.27        44\n",
      "           1       0.82      0.60      0.69       196\n",
      "\n",
      "    accuracy                           0.57       240\n",
      "   macro avg       0.51      0.51      0.48       240\n",
      "weighted avg       0.71      0.57      0.61       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  log_rfe_up_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      1.00      0.32        44\n",
      "           1       1.00      0.03      0.06       196\n",
      "\n",
      "    accuracy                           0.21       240\n",
      "   macro avg       0.59      0.52      0.19       240\n",
      "weighted avg       0.85      0.21      0.11       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  log_rfe_sm_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.43      0.27        44\n",
      "           1       0.82      0.60      0.69       196\n",
      "\n",
      "    accuracy                           0.57       240\n",
      "   macro avg       0.51      0.51      0.48       240\n",
      "weighted avg       0.71      0.57      0.61       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  log_rfe_down_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      1.00      0.31        44\n",
      "           1       0.00      0.00      0.00       196\n",
      "\n",
      "    accuracy                           0.18       240\n",
      "   macro avg       0.09      0.50      0.15       240\n",
      "weighted avg       0.03      0.18      0.06       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  log_rfe_tl_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.18      0.24        44\n",
      "           1       0.83      0.92      0.88       196\n",
      "\n",
      "    accuracy                           0.79       240\n",
      "   macro avg       0.59      0.55      0.56       240\n",
      "weighted avg       0.74      0.79      0.76       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  log_rfe_cc_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.73      0.33        44\n",
      "           1       0.87      0.41      0.56       196\n",
      "\n",
      "    accuracy                           0.47       240\n",
      "   macro avg       0.54      0.57      0.44       240\n",
      "weighted avg       0.75      0.47      0.51       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  linear_rfe_up_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        44\n",
      "           1       0.82      1.00      0.90       196\n",
      "\n",
      "    accuracy                           0.82       240\n",
      "   macro avg       0.41      0.50      0.45       240\n",
      "weighted avg       0.67      0.82      0.73       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  linear_rfe_sm_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.34      0.24        44\n",
      "           1       0.82      0.66      0.73       196\n",
      "\n",
      "    accuracy                           0.60       240\n",
      "   macro avg       0.50      0.50      0.49       240\n",
      "weighted avg       0.70      0.60      0.64       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  linear_rfe_down_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        44\n",
      "           1       0.82      1.00      0.90       196\n",
      "\n",
      "    accuracy                           0.82       240\n",
      "   macro avg       0.41      0.50      0.45       240\n",
      "weighted avg       0.67      0.82      0.73       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  linear_rfe_tl_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.07      0.12        44\n",
      "           1       0.82      0.97      0.89       196\n",
      "\n",
      "    accuracy                           0.81       240\n",
      "   macro avg       0.60      0.52      0.50       240\n",
      "weighted avg       0.74      0.81      0.75       240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#####  linear_rfe_cc_pred #####\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.61      0.31        44\n",
      "           1       0.85      0.48      0.62       196\n",
      "\n",
      "    accuracy                           0.51       240\n",
      "   macro avg       0.53      0.55      0.47       240\n",
      "weighted avg       0.73      0.51      0.56       240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\")\n",
    "print(\"##### knn_kbest_up_pred #####\")\n",
    "print(classification_report(y_test, knn_kbest_up_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"##### knn_kbest_sm_pred #####\")\n",
    "print(classification_report(y_test, knn_kbest_sm_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"##### knn_kbest_down_pred #####\")\n",
    "print(classification_report(y_test, knn_kbest_down_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"##### knn_kbest_tl_pred #####\")\n",
    "print(classification_report(y_test, knn_kbest_tl_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"##### knn_kbest_cc_pred  #####\")\n",
    "print(classification_report(y_test, knn_kbest_cc_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"##### rbf_kbest_up_pred #####\")\n",
    "print(classification_report(y_test, rbf_kbest_up_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"##### rbf_kbest_sm_pred #####\")\n",
    "print(classification_report(y_test, rbf_kbest_sm_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  rbf_kbest_down_pred #####\")\n",
    "print(classification_report(y_test, rbf_kbest_down_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  rbf_kbest_tl_pred #####\")\n",
    "print(classification_report(y_test, rbf_kbest_tl_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  rbf_kbest_cc_pred #####\")\n",
    "print(classification_report(y_test, rbf_kbest_cc_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  log_rfe_up_pred #####\")\n",
    "print(classification_report(y_test, log_rfe_up_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  log_rfe_sm_pred #####\")\n",
    "print(classification_report(y_test, log_rfe_sm_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  log_rfe_down_pred #####\")\n",
    "print(classification_report(y_test, log_rfe_down_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  log_rfe_tl_pred #####\")\n",
    "print(classification_report(y_test, log_rfe_tl_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  log_rfe_cc_pred #####\")\n",
    "print(classification_report(y_test, log_rfe_cc_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  linear_rfe_up_pred #####\")\n",
    "print(classification_report(y_test, linear_rfe_up_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  linear_rfe_sm_pred #####\")\n",
    "print(classification_report(y_test, linear_rfe_sm_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  linear_rfe_down_pred #####\")\n",
    "print(classification_report(y_test, linear_rfe_down_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  linear_rfe_tl_pred #####\")\n",
    "print(classification_report(y_test, linear_rfe_tl_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(\"#####  linear_rfe_cc_pred #####\")\n",
    "print(classification_report(y_test, linear_rfe_cc_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Abordagem com bagging através de random forests para superar problema de dataset desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.11      0.14        44\n",
      "           1       0.82      0.88      0.85       196\n",
      "\n",
      "    accuracy                           0.74       240\n",
      "   macro avg       0.49      0.50      0.49       240\n",
      "weighted avg       0.70      0.74      0.72       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "target = AbsenteeismAtWork['Absent']\n",
    "data = AbsenteeismAtWork.drop('Absent', 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform( data )\n",
    "\n",
    "kbest_selector = createKBestSelector(data, target, 10)\n",
    "selected = kbest_selector.transform(data)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_selected = kbest_selector.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(selected, target)\n",
    " \n",
    "pred = clf.predict(X_test_selected)\n",
    " \n",
    "print( accuracy_score(y_test, pred) )\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-Sensitive Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6083333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.27      0.20        44\n",
      "           1       0.81      0.68      0.74       196\n",
      "\n",
      "    accuracy                           0.61       240\n",
      "   macro avg       0.48      0.48      0.47       240\n",
      "weighted avg       0.69      0.61      0.64       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svc = SVC(kernel='linear', \n",
    "            class_weight='balanced', \n",
    "            probability=True)\n",
    "\n",
    "svc.fit(selected, target)\n",
    "\n",
    "predSvc = svc.predict(X_test_selected)\n",
    " \n",
    "print( accuracy_score(y_test, predSvc) )\n",
    "print(classification_report(y_test, predSvc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.14      0.16        44\n",
      "           1       0.82      0.87      0.84       196\n",
      "\n",
      "    accuracy                           0.74       240\n",
      "   macro avg       0.51      0.50      0.50       240\n",
      "weighted avg       0.70      0.74      0.72       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "target = AbsenteeismAtWork['Absent']\n",
    "data = AbsenteeismAtWork.drop('Absent', 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform( data )\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200\n",
    ")\n",
    "\n",
    "\n",
    "classifier.fit(data, target)\n",
    "\n",
    "predAda = classifier.predict(X_test_scaled)\n",
    " \n",
    "print( accuracy_score(y_test, predAda) )\n",
    "print(classification_report(y_test, predAda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
