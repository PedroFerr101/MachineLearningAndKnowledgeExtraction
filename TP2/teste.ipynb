{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsenteeismAtWork = pd.read_csv('data/train_data.csv', index_col=0)\n",
    "AbsenteeismAtWork['Work load Average/day '] = [x.replace(',', '') for x in AbsenteeismAtWork['Work load Average/day ']]\n",
    "AbsenteeismAtWork['Work load Average/day '] = AbsenteeismAtWork['Work load Average/day '].astype(int)\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformBalanceSelectTrainPredict( transformer, balancer, featureSelector, model, name):\n",
    "    X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "    y_train =  AbsenteeismAtWork['Absent']\n",
    "    X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "    y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "    # Normalizar, discretizar ou standardizar\n",
    "    X_train_transformed, X_test_transformed = transformer(X_train, X_test)\n",
    "    \n",
    "    # Balancear Data Set\n",
    "    X_train_balanced, y_train_balanced = balancer(X_train_transformed, y_train)\n",
    "    \n",
    "    # Feature Selection\n",
    "    X_train_selected, X_test_selected = featureSelector(X_train_balanced, y_train_balanced, X_test_transformed)\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train_selected, y_train_balanced)\n",
    "    \n",
    "    # Prever resultados para test set\n",
    "    predicted = model.predict(X_test_selected)\n",
    "    \n",
    "    # Avaliar modelo\n",
    "    evaluateModel(name, y_test, predicted)\n",
    "    return;\n",
    "\n",
    "def evaluateModel(name, y_test, predicted):\n",
    "    print(\"\".join([\"Precision 0: \",'%.3f' % precision_score(y_test,predicted,pos_label=0),\n",
    "                   \"; Precision 1: \",'%.3f' % precision_score(y_test,predicted,pos_label=1),\n",
    "                   \"; Recall 0: \",'%.3f' % recall_score(y_test,predicted,pos_label=0),\n",
    "                   \"; Recall 1: \",'%.3f' % recall_score(y_test,predicted,pos_label=1),\n",
    "                  \"; Accuracy: \",'%.3f' % accuracy_score(y_test,predicted), \"; -> \" , name]))\n",
    "    return;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up-sample minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample with replacement\n",
    "\n",
    "Método mais simples que consiste em replicar aleatoriamente (com reposição) dados da classe minoritária até atingir ratio de 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def overSampler(X_train, y_train):\n",
    "    ros = RandomOverSampler()\n",
    "    X_balanced, y_train = ros.fit_sample(X_train, y_train)\n",
    "    X_balanced, y_train = shuffle(X_balanced, y_train)\n",
    "    return X_balanced, y_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE - Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "\n",
    "def smoteSampler(X_train, y_train):\n",
    "    smote = SMOTE(sampling_strategy='minority')\n",
    "    X_balanced, y_train = smote.fit_sample(X_train, y_train)\n",
    "    X_balanced, y_train = shuffle(X_balanced, y_train)\n",
    "    return X_balanced, y_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sample majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "def underSampler(X_train, y_train):\n",
    "    rus = RandomUnderSampler()\n",
    "    X_balanced, y_train = rus.fit_sample(X_train, y_train)\n",
    "    X_balanced, y_train = shuffle(X_balanced, y_train)\n",
    "    return X_balanced, y_train;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "def tomekSampler(X_train, y_train):\n",
    "    tl = TomekLinks(sampling_strategy='majority')\n",
    "    X_balanced, y_train = tl.fit_sample(X_train, y_train)\n",
    "    X_balanced, y_train = shuffle(X_balanced, y_train)\n",
    "    return X_balanced, y_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "def centroidSampler(X_train, y_train):\n",
    "    cc = ClusterCentroids(sampling_strategy='majority')\n",
    "    X_balanced, y_train = cc.fit_sample(X_train, y_train)\n",
    "    X_balanced, y_train = shuffle(X_balanced, y_train)\n",
    "    return X_balanced, y_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mantidas:\n",
      "\t(0, 'Reason for absence')\n",
      "\t(3, 'Seasons')\n",
      "\t(5, 'Distance from Residence to Work')\n",
      "\t(7, 'Age')\n",
      "\t(9, 'Hit target')\n",
      "\t(10, 'Disciplinary failure')\n",
      "\t(12, 'Son')\n",
      "\t(16, 'Weight')\n",
      "\t(18, 'Body mass index')\n",
      "Features eliminadas:\n",
      "\t(1, 'Month of absence')\n",
      "\t(2, 'Day of the week')\n",
      "\t(4, 'Transportation expense')\n",
      "\t(6, 'Service time')\n",
      "\t(8, 'Work load Average/day ')\n",
      "\t(11, 'Education')\n",
      "\t(13, 'Social drinker')\n",
      "\t(14, 'Social smoker')\n",
      "\t(15, 'Pet')\n",
      "\t(17, 'Height')\n",
      "Precision 0: 0.295; Precision 1: 0.786; Recall 0: 0.236; Recall 1: 0.832; Accuracy: 0.696; -> Teste\n",
      "[0.3810464  0.06544625 0.06493916 0.03041194 0.05224207 0.03757482\n",
      " 0.03673212 0.04664933 0.08262405 0.08278955 0.00341362 0.01372688\n",
      " 0.01600487 0.00046129 0.00443312 0.01966774 0.00301973 0.03112693\n",
      " 0.02769015]\n",
      "Precision 0: 0.114; Precision 1: 0.929; Recall 0: 0.263; Recall 1: 0.824; Accuracy: 0.779; -> gbc\n",
      "Precision 0: 0.136; Precision 1: 0.883; Recall 0: 0.207; Recall 1: 0.820; Accuracy: 0.746; -> rfg\n",
      "Precision 0: 0.182; Precision 1: 0.852; Recall 0: 0.216; Recall 1: 0.823; Accuracy: 0.729; -> bbc\n",
      "Precision 0: 0.250; Precision 1: 0.735; Recall 0: 0.175; Recall 1: 0.814; Accuracy: 0.646; -> brf\n",
      "Precision 0: 0.273; Precision 1: 0.791; Recall 0: 0.226; Recall 1: 0.829; Accuracy: 0.696; -> rusboost\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def discretize(X_train, X_test):\n",
    "    featuresToDiscretize = ['Transportation expense', 'Distance from Residence to Work', 'Service time', 'Age', 'Work load Average/day ', 'Hit target', 'Weight', 'Height', 'Body mass index']\n",
    "    discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "    X_train[featuresToDiscretize] = discretizer.fit_transform(X_train[featuresToDiscretize])\n",
    "    X_test[featuresToDiscretize] = discretizer.transform(X_test[featuresToDiscretize])\n",
    "    return X_train, X_test;\n",
    "\n",
    "def selectKBest(X_train, y_train, X_test):\n",
    "    kbest_selector = SelectPercentile(f_classif, percentile=45)\n",
    "    selector = kbest_selector.fit(X_train, y_train)\n",
    "    printFeatureSelection(selector, X_train)\n",
    "    X_train_selected = kbest_selector.transform(X_train)\n",
    "    X_test_selected = kbest_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "X_train, y_train = overSampler(X_train, y_train)\n",
    "\n",
    "X1, X2 = discretize(X_train, X_test)\n",
    "X1, X2 = selectKBest(X1, y_train, X2)\n",
    "clf = CategoricalNB()\n",
    "clf.fit(X1, y_train)\n",
    "predicted = clf.predict(X2)\n",
    "evaluateModel(\"Teste\", predicted, y_test)\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train,X_test = discretize(X_train, X_test)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=10).fit(X_train, y_train)\n",
    "pred2 = clf.predict(X_test)\n",
    "print(clf.feature_importances_)\n",
    "evaluateModel(\"gbc\", pred2, y_test)\n",
    "\n",
    "\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "X_train, y_train = smote_tomek.fit_resample(X_train, y_train)\n",
    "X_train, X_test = discretize(X_train, X_test)\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=0, oob_score=True, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "evaluateModel(\"rfg\", y_pred, y_test)\n",
    "\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "X_train, X_test = discretize(X_train, X_test)\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "bbc = BalancedBaggingClassifier(base_estimator=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=10),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=0)\n",
    "bbc.fit(X_train, y_train) \n",
    "y_pred = bbc.predict(X_test)\n",
    "evaluateModel(\"bbc\", y_pred, y_test)\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)\n",
    "brf.fit(X_train, y_train) \n",
    "y_pred = brf.predict(X_test)\n",
    "evaluateModel(\"brf\", y_pred, y_test)\n",
    "brf.feature_importances_  \n",
    "\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "X_train, X_test = discretize(X_train, X_test)\n",
    "\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "rusboost = RUSBoostClassifier(n_estimators=2000, algorithm='SAMME.R',\n",
    "                              random_state=0)\n",
    "rusboost.fit(X_train, y_train)  \n",
    "y_pred = rusboost.predict(X_test)\n",
    "evaluateModel(\"rusboost\", y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scalerFunc(X_train, X_test): \n",
    "    print(type(X_train))\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform( X_train )\n",
    "    X_test_transformed = scaler.transform( X_test )\n",
    "    print(scaled_data)\n",
    "    return scaled_data, X_test_transformed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPickedFeatures(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    selected_features_names = list(data.columns[selected_features_index])\n",
    "    return selected_features_names;\n",
    "\n",
    "\n",
    "def getDroppedFeatures(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    dropped_features_names = list(data.columns[dropped_features_index])\n",
    "    return dropped_features_names;\n",
    "\n",
    "\n",
    "def printFeatureSelection(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    selected_features_names = zip(selected_features_index,  list(data.columns[selected_features_index]))\n",
    "    dropped_features_names = zip(dropped_features_index, list(data.columns[dropped_features_index]))\n",
    "\n",
    "    print(\"Features mantidas:\")\n",
    "    for cn in selected_features_names:\n",
    "        print(\"\\t\" + str(cn))\n",
    "\n",
    "    print(\"Features eliminadas:\")\n",
    "    for cn in dropped_features_names:\n",
    "        print(\"\\t\" + str(cn))\n",
    "    return;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectKBest (Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def selectKBest(X_train, y_train, X_test):\n",
    "    kbest_selector = SelectPercentile(f_classif, percentile=30)\n",
    "    selector = kbest_selector.fit(X_train, y_train)\n",
    "    printFeatureSelection(selector, X_train)\n",
    "    X_train_selected = kbest_selector.transform(X_train)\n",
    "    X_test_selected = kbest_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination (Wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def rfeLogReg(X_train, y_train, X_test):\n",
    "    rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "    rfe_log_selector = rfe_log_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = rfe_log_selector.transform(X_train)\n",
    "    X_test_selected = rfe_log_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;\n",
    "\n",
    "def rfeSVC(X_train, y_train, X_test):\n",
    "    rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "    rfe_svc_selector = rfe_svc_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = rfe_svc_selector.transform(X_train)\n",
    "    X_test_selected = rfe_svc_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluate different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.175; Precision 1: 0.814; Recall 0: 0.227; Recall 1: 0.760; Accuracy: 0.662; -> test\n",
      "Precision 0: 0.250; Precision 1: 0.841; Recall 0: 0.364; Recall 1: 0.755; Accuracy: 0.683; -> test\n",
      "Precision 0: 0.215; Precision 1: 0.837; Recall 0: 0.455; Recall 1: 0.628; Accuracy: 0.596; -> test\n",
      "Precision 0: 0.217; Precision 1: 0.820; Recall 0: 0.114; Recall 1: 0.908; Accuracy: 0.762; -> test\n",
      "Precision 0: 0.217; Precision 1: 0.834; Recall 0: 0.409; Recall 1: 0.668; Accuracy: 0.621; -> test\n",
      "Precision 0: 0.185; Precision 1: 0.817; Recall 0: 0.273; Recall 1: 0.730; Accuracy: 0.646; -> test\n",
      "Precision 0: 0.205; Precision 1: 0.826; Recall 0: 0.341; Recall 1: 0.704; Accuracy: 0.637; -> test\n",
      "Precision 0: 0.211; Precision 1: 0.822; Recall 0: 0.182; Recall 1: 0.847; Accuracy: 0.725; -> test\n",
      "Precision 0: 0.312; Precision 1: 0.826; Recall 0: 0.114; Recall 1: 0.944; Accuracy: 0.792; -> test\n",
      "Precision 0: 0.208; Precision 1: 0.833; Recall 0: 0.455; Recall 1: 0.612; Accuracy: 0.583; -> test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.226; Precision 1: 0.829; Recall 0: 0.273; Recall 1: 0.791; Accuracy: 0.696; -> test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.211; Precision 1: 0.825; Recall 0: 0.273; Recall 1: 0.770; Accuracy: 0.679; -> test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.223; Precision 1: 0.842; Recall 0: 0.477; Recall 1: 0.628; Accuracy: 0.600; -> test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.381; Precision 1: 0.836; Recall 0: 0.182; Recall 1: 0.934; Accuracy: 0.796; -> test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0: 0.195; Precision 1: 0.832; Recall 0: 0.591; Recall 1: 0.454; Accuracy: 0.479; -> test\n",
      "Precision 0: 0.222; Precision 1: 0.848; Recall 0: 0.545; Recall 1: 0.571; Accuracy: 0.567; -> test\n",
      "Precision 0: 0.159; Precision 1: 0.811; Recall 0: 0.159; Recall 1: 0.811; Accuracy: 0.692; -> test\n",
      "Precision 0: 0.221; Precision 1: 0.838; Recall 0: 0.432; Recall 1: 0.658; Accuracy: 0.617; -> test\n",
      "Precision 0: 0.375; Precision 1: 0.823; Recall 0: 0.068; Recall 1: 0.974; Accuracy: 0.808; -> test\n",
      "Precision 0: 0.189; Precision 1: 0.824; Recall 0: 0.568; Recall 1: 0.454; Accuracy: 0.475; -> test\n",
      "Precision 0: 0.189; Precision 1: 0.819; Recall 0: 0.318; Recall 1: 0.694; Accuracy: 0.625; -> test\n",
      "Precision 0: 0.132; Precision 1: 0.793; Recall 0: 0.227; Recall 1: 0.663; Accuracy: 0.583; -> test\n",
      "Precision 0: 0.217; Precision 1: 0.834; Recall 0: 0.409; Recall 1: 0.668; Accuracy: 0.621; -> test\n",
      "Precision 0: 0.294; Precision 1: 0.825; Recall 0: 0.114; Recall 1: 0.939; Accuracy: 0.787; -> test\n",
      "Precision 0: 0.188; Precision 1: 0.819; Recall 0: 0.341; Recall 1: 0.668; Accuracy: 0.608; -> test\n",
      "Precision 0: 0.211; Precision 1: 0.828; Recall 0: 0.341; Recall 1: 0.714; Accuracy: 0.646; -> test\n",
      "Precision 0: 0.193; Precision 1: 0.820; Recall 0: 0.250; Recall 1: 0.765; Accuracy: 0.671; -> test\n",
      "Precision 0: 0.232; Precision 1: 0.842; Recall 0: 0.432; Recall 1: 0.679; Accuracy: 0.633; -> test\n",
      "Precision 0: 0.375; Precision 1: 0.823; Recall 0: 0.068; Recall 1: 0.974; Accuracy: 0.808; -> test\n",
      "Precision 0: 0.203; Precision 1: 0.825; Recall 0: 0.318; Recall 1: 0.719; Accuracy: 0.646; -> test\n",
      "Precision 0: 0.212; Precision 1: 0.831; Recall 0: 0.386; Recall 1: 0.679; Accuracy: 0.625; -> test\n",
      "Precision 0: 0.225; Precision 1: 0.841; Recall 0: 0.455; Recall 1: 0.648; Accuracy: 0.613; -> test\n",
      "Precision 0: 0.200; Precision 1: 0.831; Recall 0: 0.500; Recall 1: 0.551; Accuracy: 0.542; -> test\n",
      "Precision 0: 0.368; Precision 1: 0.833; Recall 0: 0.159; Recall 1: 0.939; Accuracy: 0.796; -> test\n",
      "Precision 0: 0.209; Precision 1: 0.838; Recall 0: 0.523; Recall 1: 0.556; Accuracy: 0.550; -> test\n",
      "Precision 0: 0.225; Precision 1: 0.838; Recall 0: 0.409; Recall 1: 0.684; Accuracy: 0.633; -> test\n",
      "Precision 0: 0.222; Precision 1: 0.844; Recall 0: 0.500; Recall 1: 0.607; Accuracy: 0.588; -> test\n",
      "Precision 0: 0.240; Precision 1: 0.857; Recall 0: 0.545; Recall 1: 0.612; Accuracy: 0.600; -> test\n",
      "Precision 0: 0.375; Precision 1: 0.823; Recall 0: 0.068; Recall 1: 0.974; Accuracy: 0.808; -> test\n",
      "Precision 0: 0.195; Precision 1: 0.829; Recall 0: 0.545; Recall 1: 0.495; Accuracy: 0.504; -> test\n"
     ]
    }
   ],
   "source": [
    "# Transformer : discretize , scalerFunc\n",
    "# Balancer: overSampler , smoteSampler , underSampler , tomekSampler , centroidSampler\n",
    "# FeatureSelector : selectKBest , rfeLogReg , rfeSVC\n",
    "\n",
    "TransformBalanceSelectTrainPredict( discretize, overSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, smoteSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, underSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, tomekSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, centroidSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "\n",
    "TransformBalanceSelectTrainPredict( discretize, overSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, smoteSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, underSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, tomekSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, centroidSampler, selectKBest, SVC(), \"test\")\n",
    "\n",
    "\n",
    "TransformBalanceSelectTrainPredict( discretize, overSampler, rfeLogReg,  LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, smoteSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, underSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, tomekSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, centroidSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "\n",
    "\n",
    "TransformBalanceSelectTrainPredict( discretize, overSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, smoteSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, underSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, tomekSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( discretize, centroidSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "\n",
    "\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, overSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, smoteSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, underSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, tomekSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, centroidSampler, selectKBest, KNeighborsClassifier(n_neighbors=5), \"test\")\n",
    "\n",
    "\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, overSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, smoteSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, underSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, tomekSampler, selectKBest, SVC(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, centroidSampler, selectKBest, SVC(), \"test\")\n",
    "\n",
    "\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, overSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, smoteSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, underSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, tomekSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, centroidSampler, rfeLogReg, LogisticRegression(), \"test\")\n",
    "\n",
    "\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, overSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, smoteSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, underSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, tomekSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n",
    "TransformBalanceSelectTrainPredict( scalerFunc, centroidSampler, rfeSVC, SVC(kernel='linear'), \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Abordagem com bagging através de random forests para superar problema de dataset desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.11      0.16        44\n",
      "           1       0.82      0.93      0.88       196\n",
      "\n",
      "    accuracy                           0.78       240\n",
      "   macro avg       0.55      0.52      0.52       240\n",
      "weighted avg       0.72      0.78      0.74       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "target = AbsenteeismAtWork['Absent']\n",
    "data = AbsenteeismAtWork.drop('Absent', 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform( data )\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(data, target)\n",
    " \n",
    "pred = clf.predict(X_test_scaled)\n",
    " \n",
    "print( accuracy_score(y_test, pred) )\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-Sensitive Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0e77ebdce4be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             probability=True)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpredSvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selected' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svc = SVC(kernel='linear', \n",
    "            class_weight='balanced', \n",
    "            probability=True)\n",
    "\n",
    "svc.fit(selected, target)\n",
    "\n",
    "predSvc = svc.predict(X_test_selected)\n",
    " \n",
    "print( accuracy_score(y_test, predSvc) )\n",
    "print(classification_report(y_test, predSvc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7208333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.14      0.15        44\n",
      "           1       0.81      0.85      0.83       196\n",
      "\n",
      "    accuracy                           0.72       240\n",
      "   macro avg       0.49      0.49      0.49       240\n",
      "weighted avg       0.70      0.72      0.71       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "target = AbsenteeismAtWork['Absent']\n",
    "data = AbsenteeismAtWork.drop('Absent', 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform( data )\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200\n",
    ")\n",
    "\n",
    "\n",
    "classifier.fit(data, target)\n",
    "\n",
    "predAda = classifier.predict(X_test_scaled)\n",
    " \n",
    "print( accuracy_score(y_test, predAda) )\n",
    "print(classification_report(y_test, predAda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
