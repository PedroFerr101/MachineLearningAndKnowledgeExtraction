{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, GenericUnivariateSelect\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsenteeismAtWork = pd.read_csv('data/train_data.csv', index_col=0)\n",
    "AbsenteeismAtWork['Work load Average/day '] = [x.replace(',', '') for x in AbsenteeismAtWork['Work load Average/day ']]\n",
    "AbsenteeismAtWork['Work load Average/day '] = AbsenteeismAtWork['Work load Average/day '].astype(int)\n",
    "\n",
    "X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "y_train =  AbsenteeismAtWork['Absent']\n",
    "X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustScaling2(X_train, X_test):\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data = scaler.fit_transform( X_train )\n",
    "    scaled_test = scaler.transform( X_test )\n",
    "    return scaled_data, scaled_test;\n",
    "\n",
    "def evaluateSelector(featureSelector, selectorName):\n",
    "    X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "    y_train =  AbsenteeismAtWork['Absent']\n",
    "    X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "    y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "    \n",
    "    # Feature Selection\n",
    "    X_train_selected, X_test_selected = featureSelector(X_train, y_train, X_test)\n",
    "    \n",
    "    # standardizar\n",
    "    X_train_transformed, X_test_transformed = robustScaling2(X_train_selected, X_test_selected)\n",
    "    \n",
    "    \n",
    "    classifiers = [\n",
    "        LogisticRegression(),\n",
    "        SGDClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors=5),\n",
    "        SVC(),\n",
    "        LinearSVC(max_iter=10000),\n",
    "        GaussianNB(),\n",
    "        GaussianProcessClassifier(),\n",
    "        DecisionTreeClassifier(),\n",
    "        MLPClassifier(max_iter=10000),\n",
    "        AdaBoostClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "    ]\n",
    "    \n",
    "    names = [\n",
    "             \"Logistic regression\", \"SGDClassifier\",\n",
    "             \"KNearest Neighbors (5)\", \n",
    "             \"SVM-rbf\", \"SMV-linear\", \n",
    "             \"Gaussian naive bayes\",\n",
    "             \"Gaussian Process\", \n",
    "             \"Decision Tree\", \n",
    "             \"Multi-layer Perceptron\", \n",
    "             \"AdaBoost\", \"Random Forest\"]\n",
    "    \n",
    "    # Treinar e avaliar modelos\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train_transformed, y_train)\n",
    "        predicted = clf.predict(X_test_transformed)\n",
    "        evaluateModel(name + \" com \" + selectorName, y_test, predicted)\n",
    "    return;\n",
    "\n",
    "\n",
    "def evaluateModelBasedSelector(featureSelector, model, name):\n",
    "    X_train = AbsenteeismAtWork.drop('Absent', 1)\n",
    "    y_train =  AbsenteeismAtWork['Absent']\n",
    "    X_test = pd.read_csv('data/test_data.csv', index_col=0)\n",
    "    y_test = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "\n",
    "    # Feature Selection\n",
    "    X_train_selected, X_test_selected = featureSelector(X_train, y_train, X_test)\n",
    "    \n",
    "    \n",
    "    # Normalizar, discretizar ou standardizar\n",
    "    X_train_transformed, X_test_transformed = robustScaling2(X_train_selected, X_test_selected)\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Prever resultados para test set\n",
    "    predicted = model.predict(X_test_transformed)\n",
    "    \n",
    "    # Avaliar modelo\n",
    "    evaluateModel(name, y_test, predicted)\n",
    "    return;\n",
    "\n",
    "def evaluateModel(name, y_test, predicted):\n",
    "    print(\"Accuracy: %0.3f || AUROC %0.3f || (Accuracy, Precision) 0:( %0.3f, %0.3f)  1:( %0.3f, %0.3f) ->\" \n",
    "              % (accuracy_score(y_test,predicted), roc_auc_score(y_test, predicted),\n",
    "                recall_score(y_test,predicted,pos_label=0), precision_score(y_test,predicted,pos_label=0),\n",
    "                recall_score(y_test,predicted,pos_label=1), precision_score(y_test,predicted,pos_label=1)), name)\n",
    "    return;\n",
    "\n",
    "\n",
    "def getPickedFeatures(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    selected_features_names = list(data.columns[selected_features_index])\n",
    "    return selected_features_names;\n",
    "\n",
    "\n",
    "def getDroppedFeatures(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    dropped_features_names = list(data.columns[dropped_features_index])\n",
    "    return dropped_features_names;\n",
    "\n",
    "\n",
    "def printFeatureSelection(selector, data):\n",
    "    selected_features_index = selector.get_support(indices=True)\n",
    "    dropped_features_index = list( set(list(range(0, data.columns.size))) - (set(selected_features_index)))\n",
    "\n",
    "    selected_features_names = zip(selected_features_index,  list(data.columns[selected_features_index]))\n",
    "    dropped_features_names = zip(dropped_features_index, list(data.columns[dropped_features_index]))\n",
    "\n",
    "    print(\"Features mantidas:\")\n",
    "    for cn in selected_features_names:\n",
    "        print(\"\\t\" + str(cn))\n",
    "\n",
    "    print(\"Features eliminadas:\")\n",
    "    for cn in dropped_features_names:\n",
    "        print(\"\\t\" + str(cn))\n",
    "    return;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectVarianceThreshold(X_train, y_train, X_test):\n",
    "    varianceThreshold_selector = VarianceThreshold()\n",
    "    selector = varianceThreshold_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_train)\n",
    "    X_train_selected = varianceThreshold_selector.transform(X_train)\n",
    "    X_test_selected = varianceThreshold_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectKBest_f_classif(X_train, y_train, X_test):\n",
    "    kbest_selector_f_classif = SelectKBest(f_classif, k=12)\n",
    "    selector = kbest_selector_f_classif.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_train)\n",
    "    X_train_selected = kbest_selector_f_classif.transform(X_train)\n",
    "    X_test_selected = kbest_selector_f_classif.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;\n",
    "\n",
    "def selectKBest_chi2(X_train, y_train, X_test):\n",
    "    kbest_selector_chi2 = SelectKBest(chi2, k=12)\n",
    "    selector = kbest_selector_chi2.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_train)\n",
    "    X_train_selected = kbest_selector_chi2.transform(X_train)\n",
    "    X_test_selected = kbest_selector_chi2.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectPercentile_f_classif(X_train, y_train, X_test):\n",
    "    percentile_selector_f_classif = SelectPercentile(f_classif, percentile=10)\n",
    "    selector = percentile_selector_f_classif.fit(X_train, y_train)\n",
    "    X_train_selected = percentile_selector_f_classif.transform(X_train)\n",
    "    X_test_selected = percentile_selector_f_classif.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;\n",
    "\n",
    "def selectPercentile_chi2(X_train, y_train, X_test):\n",
    "    percentile_selector_chi2 = SelectPercentile(chi2, percentile=10)\n",
    "    selector = percentile_selector_chi2.fit(X_train, y_train)\n",
    "    X_train_selected = percentile_selector_chi2.transform(X_train)\n",
    "    X_test_selected = percentile_selector_chi2.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GenericUnivariateSelect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectGenericUnivariateSelect(X_train, y_train, X_test):\n",
    "    gus_selector = GenericUnivariateSelect(f_classif, 'k_best', param=19)\n",
    "    selector = gus_selector.fit(X_train, y_train)\n",
    "    X_train_selected = gus_selector.transform(X_train)\n",
    "    X_test_selected = gus_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfeLogReg(X_train, y_train, X_test):\n",
    "    rfe_log_selector = RFE(LogisticRegression(), 12)\n",
    "    selector = rfe_log_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = rfe_log_selector.transform(X_train)\n",
    "    X_test_selected = rfe_log_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;\n",
    "\n",
    "\n",
    "def rfeSVC(X_train, y_train, X_test):\n",
    "    rfe_svc_selector = RFE(SVC(kernel='linear'), 12)\n",
    "    selector = rfe_svc_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = rfe_svc_selector.transform(X_train)\n",
    "    X_test_selected = rfe_svc_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination w/ Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfeCvLogReg(X_train, y_train, X_test):\n",
    "    rfecv_log_selector = RFECV(LogisticRegression(), 12)\n",
    "    selector = rfecv_log_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = rfecv_log_selector.transform(X_train)\n",
    "    X_test_selected = rfecv_log_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;\n",
    "\n",
    "def rfeCvSVC(X_train, y_train, X_test):\n",
    "    rfecv_svc_selector = RFECV(SVC(kernel='linear'), 12)\n",
    "    selector = rfecv_svc_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = rfecv_svc_selector.transform(X_train)\n",
    "    X_test_selected = rfecv_svc_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfmLogReg(X_train, y_train, X_test):\n",
    "    sfm_logReg_selector = SelectFromModel(estimator=LogisticRegression())\n",
    "    selector = sfm_logReg_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = sfm_logReg_selector.transform(X_train)\n",
    "    X_test_selected = sfm_logReg_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectFromModel and LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfmLcvLogReg(X_train, y_train, X_test):\n",
    "    sfmlcv_logReg_selector = SelectFromModel(LassoCV(),threshold=0.25)\n",
    "    selector = sfmlcv_logReg_selector.fit(X_train, y_train)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = sfmlcv_logReg_selector.transform(X_train)\n",
    "    X_test_selected = sfmlcv_logReg_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfmL1(X_train, y_train, X_test):\n",
    "    lsvc_selector = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\n",
    "    selector = lsvc_selector.fit(X_train, y_train)\n",
    "    l1_selector = SelectFromModel(lsvc_selector, prefit=True)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = l1_selector.transform(X_train)\n",
    "    X_test_selected = l1_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfmTree(X_train, y_train, X_test):\n",
    "    tree_selector = ExtraTreesClassifier(n_estimators=50)\n",
    "    selector = tree_selector.fit(X_train, y_train)\n",
    "    sfm_Tree_selector = SelectFromModel(tree_selector, prefit=True)\n",
    "    #printFeatureSelection(selector, X_balanced)\n",
    "    X_train_selected = sfm_Tree_selector.transform(X_train)\n",
    "    X_test_selected = sfm_Tree_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluate different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.804 || AUROC 0.545 || (Accuracy, Precision) 0:( 0.136, 0.400)  1:( 0.954, 0.831) -> Logistic regression com VarianceThreshold\n",
      "Accuracy: 0.792 || AUROC 0.546 || (Accuracy, Precision) 0:( 0.159, 0.350)  1:( 0.934, 0.832) -> SGDClassifier com VarianceThreshold\n",
      "Accuracy: 0.804 || AUROC 0.501 || (Accuracy, Precision) 0:( 0.023, 0.200)  1:( 0.980, 0.817) -> KNearest Neighbors (5) com VarianceThreshold\n",
      "Accuracy: 0.812 || AUROC 0.524 || (Accuracy, Precision) 0:( 0.068, 0.429)  1:( 0.980, 0.824) -> SVM-rbf com VarianceThreshold\n",
      "Accuracy: 0.804 || AUROC 0.545 || (Accuracy, Precision) 0:( 0.136, 0.400)  1:( 0.954, 0.831) -> SMV-linear com VarianceThreshold\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Gaussian naive bayes com VarianceThreshold\n",
      "Accuracy: 0.796 || AUROC 0.514 || (Accuracy, Precision) 0:( 0.068, 0.273)  1:( 0.959, 0.821) -> Gaussian Process com VarianceThreshold\n",
      "Accuracy: 0.738 || AUROC 0.566 || (Accuracy, Precision) 0:( 0.295, 0.289)  1:( 0.837, 0.841) -> Decision Tree com VarianceThreshold\n",
      "Accuracy: 0.729 || AUROC 0.499 || (Accuracy, Precision) 0:( 0.136, 0.182)  1:( 0.862, 0.816) -> Multi-layer Perceptron com VarianceThreshold\n",
      "Accuracy: 0.767 || AUROC 0.513 || (Accuracy, Precision) 0:( 0.114, 0.227)  1:( 0.913, 0.821) -> AdaBoost com VarianceThreshold\n",
      "Accuracy: 0.775 || AUROC 0.510 || (Accuracy, Precision) 0:( 0.091, 0.222)  1:( 0.929, 0.820) -> Random Forest com VarianceThreshold\n"
     ]
    }
   ],
   "source": [
    "# Removing features with low variance\n",
    "\n",
    "evaluateSelector(selectVarianceThreshold, \"VarianceThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate feature selection\n",
    "print(\"Com f_classif\\n\\n\")\n",
    "evaluateSelector(selectKBest_f_classif, \"Select k best f_classif\")\n",
    "\n",
    "print(\"Com qui2\\n\\n\")\n",
    "evaluateSelector(selectKBest_chi2, \"Select k best qui2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Logistic regression com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> SGDClassifier com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> KNearest Neighbors (5) com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> SVM-rbf com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> SMV-linear com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Gaussian naive bayes com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Gaussian Process com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Decision Tree com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Multi-layer Perceptron com Select Percentile\n",
      "Accuracy: 0.804 || AUROC 0.528 || (Accuracy, Precision) 0:( 0.091, 0.364)  1:( 0.964, 0.825) -> AdaBoost com Select Percentile\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Random Forest com Select Percentile\n"
     ]
    }
   ],
   "source": [
    "print(\"Com f_classif\\n\\n\")\n",
    "evaluateSelector(selectPercentile_f_classif, \"Select Percentile\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.804 || AUROC 0.545 || (Accuracy, Precision) 0:( 0.136, 0.400)  1:( 0.954, 0.831) -> Logistic regression com Generic univariate select\n",
      "Accuracy: 0.775 || AUROC 0.519 || (Accuracy, Precision) 0:( 0.114, 0.250)  1:( 0.923, 0.823) -> SGDClassifier com Generic univariate select\n",
      "Accuracy: 0.804 || AUROC 0.501 || (Accuracy, Precision) 0:( 0.023, 0.200)  1:( 0.980, 0.817) -> KNearest Neighbors (5) com Generic univariate select\n",
      "Accuracy: 0.812 || AUROC 0.524 || (Accuracy, Precision) 0:( 0.068, 0.429)  1:( 0.980, 0.824) -> SVM-rbf com Generic univariate select\n",
      "Accuracy: 0.804 || AUROC 0.545 || (Accuracy, Precision) 0:( 0.136, 0.400)  1:( 0.954, 0.831) -> SMV-linear com Generic univariate select\n",
      "Accuracy: 0.808 || AUROC 0.521 || (Accuracy, Precision) 0:( 0.068, 0.375)  1:( 0.974, 0.823) -> Gaussian naive bayes com Generic univariate select\n",
      "Accuracy: 0.796 || AUROC 0.514 || (Accuracy, Precision) 0:( 0.068, 0.273)  1:( 0.959, 0.821) -> Gaussian Process com Generic univariate select\n",
      "Accuracy: 0.754 || AUROC 0.576 || (Accuracy, Precision) 0:( 0.295, 0.317)  1:( 0.857, 0.844) -> Decision Tree com Generic univariate select\n",
      "Accuracy: 0.750 || AUROC 0.503 || (Accuracy, Precision) 0:( 0.114, 0.192)  1:( 0.893, 0.818) -> Multi-layer Perceptron com Generic univariate select\n",
      "Accuracy: 0.767 || AUROC 0.513 || (Accuracy, Precision) 0:( 0.114, 0.227)  1:( 0.913, 0.821) -> AdaBoost com Generic univariate select\n",
      "Accuracy: 0.783 || AUROC 0.515 || (Accuracy, Precision) 0:( 0.091, 0.250)  1:( 0.939, 0.821) -> Random Forest com Generic univariate select\n"
     ]
    }
   ],
   "source": [
    "evaluateSelector(selectGenericUnivariateSelect, \"Generic univariate select\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.792 || AUROC 0.511 || (Accuracy, Precision) 0:( 0.068, 0.250)  1:( 0.954, 0.820) -> Log Reg w/ RFE LogReg\n",
      "Accuracy: 0.804 || AUROC 0.545 || (Accuracy, Precision) 0:( 0.136, 0.400)  1:( 0.954, 0.831) -> Log Reg w/ RFE Cross Validation LogReg\n"
     ]
    }
   ],
   "source": [
    "# Recursive feature elimination\n",
    "\n",
    "evaluateModelBasedSelector( rfeLogReg,  LogisticRegression(), \"Log Reg w/ RFE LogReg\")\n",
    "evaluateModelBasedSelector( rfeCvLogReg, LogisticRegression(), \"Log Reg w/ RFE Cross Validation LogReg\")\n",
    "\n",
    "evaluateModelBasedSelector( rfeSVC, SVC(kernel='linear'), \"SVC Linear w/ RFE SVC\")\n",
    "evaluateModelBasedSelector( rfeCvSVC, SVC(kernel='linear'), \"SVC Linear w/ RFE Cross Validation SVC\")\n",
    "\n",
    "# Feature selection using SelectFromModel\n",
    "\n",
    "\n",
    "evaluateModelBasedSelector( sfmLcvLogReg, LogisticRegression(), \"Log Reg w/ SelectFromModel and LassoCV\")\n",
    "\n",
    "#evaluateModelBasedSelector( sfmL1, SVC(kernel='linear'), \"SVC Linear w/ SelectFromModel L1\")\n",
    "\n",
    "evaluateModelBasedSelector( sfmTree, SVC(kernel='linear'), \"SVC Linear w/ SelectFromModel Tree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
